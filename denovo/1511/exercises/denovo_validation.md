---
layout: default
title:  'Exercise: De Novo Validation'
---

##Exercise: De Novo Validation

In this exercise we will perform assembly validation of 5 assemblies. The excercise is based on real data freely available here:  
- *Staphylococcus aureus*, [GAGE dataset](http://gage.cbcb.umd.edu/data/Staphylococcus_aureus/) 

We will use 5 assemblies generated by a group of de novo assembly experts (e.g., *Salzberg S.L.*, *Philippy A.*). Have a look at http://gage.cbcb.umd.edu/. We will evaluate their assemblies using several techniques and tools and, if you want, you can add your de novo assembly and judge and compare your own results. These assemblies have been around for some time, so some of the assembliers they used are not what we'd use today - but the validation techniques are the same.

In order to facilitate the tutorial all steps have been precomputed and can be found here:

```
$ /proj/g2015027/assemblyValidation/AV_Exercise_executed
```

We will go through all the steps using the Velvet assembly. All assemblies have already been computed, or can  easily be computed using shell scripts present in each working directory. You are, however, free to rerun them completely (if time allows it) or you can run the various steps on your own assemblies.

### Setting up your project environment  

Copy the data from the project directory to your home folder in order to run your excersices locally. It is important that you copy the files exactly in this way as many commands that will follow rely on finding things in a precise place. **DO NOT START TO RUN SCRIPTS ALREADY: WE NEED TO FIX THE WORKING ENVIRONMENT FIRST.**

```
$ cd ~/glob/
$ rsync -r -v --progress /proj/g2015027/assemblyValidation/AV_Exercise/ AV_Exercise 
$ cd AV_Exercise/
```

This will be our working directory.

In `/proj/g2015027/assemblyValidation/AV_Exercise_executed/` you can find an identical directory structure with all the exercise already computed. Use this if you want to have a look at the results.
If you soft link this folder into your directory pay attention to not delete the contents (do ***_not_*** execute `rm -r AV_Exercise_executed/`).

 The AV_Exercise folder in your home contains the following elements:

- **tools**: set of useful tools and commands to facilitate your work.
- **data**: data, Paired-End reads in the folder PE (in standard format) and Mate-Pair reads in the folder MP_rc (they have been reverse complemented as software we will use expect all pairs to be innies --><--)
- **assemblies**: de novo assemblies for *ABySS*, *Allpaths-LG*, *MaSuRCA* (a.k.a. MSR-CA), *SOAPdenovo*, and *Velvet*. **Note**: In case you want to run the exercise on your own de novo assembly you must first rename all the contigs (REAPR wants all contigs in a form similar to ">contig_NUM"). Use the script `~/AV_Exercise/tools/rename_assembly_contigs.pl --assembly ORIGINAL_SEQ > RENAMED_SEQ`
- **01_assembly_statistics**: first evaluation step, i.e., standard assembly statistics.
- **02_read_mapping**: reads alignments (PE and MP) against the assemblies, this is a required step for further analysis. Running `run_alignments.sh` allows you to generate all the alignments, but this is time consuming (it takes 34 minutes), so you are encouraged to use the script `run_softlinkAlignments.sh` that creates softlinks to pre-computed alignments and runs "only" PicardTools (it takes less than 2 minutes).
- **03_QAtools**: In this folder we will execute and play around with QA-tools, a set of useful tools to asses assembly complexity/quality. The command run_QAtools.sh executes the tool.
- **04_KAT**: Here we'll take another look at KAT, a toolset for kmer analysis which can be used both for pre-assembly QC, and post-assembly validation.
- **05_FRCbam**: In this folder we will execute and play around with FRCbam a tool to rank de novo assemblies obtained with different assemblers.
- **06_REAPR**: In this directory are scripts to try out REAPR - a tool to rank and correct assemblies. As this tool is slower than the others (37 minutes to evaluate all 5 assemblies) you will find the results already computed (some partial large files have been deleted to avoid the transfer of large files).
- **07_CEGMA**: results from CEGMA. This tool is extremely slow and unstable so we will only look at the results.

For each assembler contig (*ctg*) and scaffold (*scf*) sequences are present. We will use _only_ scaffolds but everything we say can be done on contigs too.

### Setting up the working enviorment

Now all the data you need is in your home folder, on `glob`, in the `AV_Exercise` directory. You still need a lot of tools though.

Setting up the working the environment (i.e., be sure that all the needed tools are available on the command line) is the first and probably most important step. 

Copy and paste the following commands (preferably one line at a time):

```
module load bioinfo-tools
module load bwa/0.7.5a
module load samtools
module load bamtools
module load picard/1.92
FRCbam=/proj/g2015027/assemblyValidation/tools/FRCbam/src/
QAtools=/proj/g2015027/assemblyValidation/tools/qaTools/
REAPR=/proj/g2015027/assemblyValidation/tools/Reapr_1.0.16/
export PATH=$PATH:$FRCbam:$QAtools:$REAPR
export PERL5LIB=/proj/g2015027/assemblyValidation/tools/File-Copy-Link-0.112/lib/
module load BioPerl/1.6.1
```

This should set up all the variables and paths needed to run all the programs in this tutorial! Try this:

```
$ FRC --help
```

If you see a help message then everything should be working. If it's not, feel free to panic. Another solution is to call one of us and we'll sort it out.

At this point everything should be set and you can start working. Start by looking at the `data` and `assemblies` directories to see how they are structured, and what you'll be working with.

### 01 - Standard assembly statistics

Standard assembly statistics do not tell too much about assembly quality, however they are the first thing to look at as:

- they can be used to identify an extremely problematic assembler (e.g. an assembler that has thousands of contigs more than the others)
- they can be used to understand that more parameters should be changed (e.g. the "best" assembler has an NG50 of 300bp) 

**Remember**: never (*never*, **never**, ~sometimes~) use these statistics as only source to decide which assembler/assembly is the best. ***Contiguity and Correctness are not correlated***!
In this step we will generate the same statistics that's generated by QUAST, but we'll generate it on the command line. This is useful to get a quick look on the numbers if you're unable to open a report file, but using quast is generally both easier and better since it also produces graphs!

```
$ cd ~/glob/AV_Exercise/01_assembly_statistics/
$ ./assembly_stats.pl --assembly ~/glob/AV_Exercise/assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta --genome-length 2900000 > Staph_velvet.stats 
```

You can run this script on all others de novo assemblies (stored in the folder `~/AV_Excercise/assemblies`) and obtain a table similar to the one below here. The simple script called `make_table.sh` can be used to easily make a table from all the .stats-files in the directory.

The output file reports assembly statistics twice: first in a human readable way and then in a tabular form that can be easily exported to excel. 

Assembly | GenomeSize | AssemblySize | #ctgs/scfs | MaxLength | NG50 | LG50 | NG90 | LG90 
:------- | ----------:| ------------:| ----------:| ---------:| ----:| ----:| ----:| ----:
Staphylococcus_aureus.abyss.scf.fasta | 2900000 | 3821622 | 125 | 346557 | 170210 | 6 | 78132 | 17 
Staphylococcus_aureus.allpaths.scf.fasta | 2900000 | 2880676 | 19 | 1435559 | 1091731 | 2 | 179561 | 3 
Staphylococcus_aureus.masurca.scf.fasta | 2900000 | 2872905 | 17 | 2411914 | 2411914 | 1 | 155799 | 3 
Staphylococcus_aureus.soapdenovo.scf.fasta | 2900000 | 2924135 | 175 | 518710 | 331598 | 4 | 93229 | 10 
Staphylococcus_aureus.velvet.scf.fasta | 2900000 | 2877995 | 173 | 989718 | 762333 | 2 | 142854 | 5 

#### Questions 
- Which is the best assembly?
- Are there any outlier(s)?
- What about your assembly? Does it seems close to these ones? 

<hr>

One way to evaluate an assembly is to map back reads and check if they are congruent with the contigs/scaffolds. Moreover, alignments alone are able to give us a lot of informations about the assemblies and the genome content.

We will look into three tools based on this approach:

- QAtools: unpublished, available on-line on git-hub.
- REAPR: publication "REAPR: a universal tool for genome assembly evaluation"
- FRCbam: publication "Re-evaluating assembly evaluation with Feature Responses Curves"

but first of all let us generate the alignments. 

### 02 - Read Alignment

We will now see how to align both PE and MP reads against an assembly (scaffolded sequences). This step is time consuming, you can do it manually on one assembly to try the various steps. You are strongly encouraged to try this on your own assembly.

You need to work in the following directory:

```
~/AV_Exercise/02_read_mapping
```

To automatically generate all the alignments run `bash run_alignments.sh` (this will take ~30 minutes)

To softlink here pre-computed results run `bash run_softlinkAlignments.sh` (this will take ~2 minutes as PicardTools are exectued)

These are the different steps to map the reads to the assembly. I will use the "velvet" assembly, but feel free to use any of them:

Create directory and collect all data using soft links:

```
$ mkdir velvet
$ cd velvet
$ ln -s ~/AV_Exercise/assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta .
$ ln -s ~/AV_Exercise/data/PE/Staphylococcus_aureus_PE_1.fastq .
$ ln -s ~/AV_Exercise/data/PE/Staphylococcus_aureus_PE_2.fastq .
$ ln -s ~/AV_Exercise/data/MP_rc/Staphylococcus_aureus_MP_rc_1.fastq .
$ ln -s ~/AV_Exercise/data/MP_rc/Staphylococcus_aureus_MP_rc_2.fastq .
```

Now assembly and reads are available from our local directory. **Note**: we are using soft-links - and this is highly recommended in order to avoid using too much space. Have a backup copy, and one working copy, that's all you'll ever need!

Now create index:

```
$ bwa index Staphylococcus_aureus.velvet.scf.fasta
```

Now we have the index, so we can align reads with bwa. We start with the PE:

```
$ bwa mem -t 8 -M Staphylococcus_aureus.velvet.scf.fasta  Staphylococcus_aureus_PE_1.fastq Staphylococcus_aureus_PE_2.fastq > velvet_PE.sam
```

Now convert SAM file into BAM file (the binary BAM format uses less space, and more tools are able to use it):

```
$ samtools view -Sb velvet_PE.sam -o velvet_PE.bam 
```

Now sort the BAM file (many tools require it sorted):

```
$ samtools sort velvet_PE.bam velvet_PE.sorted
```

Finally, index the BAM file (makes reading it faster, and many tools require it indexed):

```
$ samtools index velvet_PE.sorted.bam
```

Now compute Insert Size Statistics with PicardTools (extremely usefull suite of tools, but can be a bit tricky to use):

```
$ java -Xmx16g -XX:PermSize=8g -jar $PICARD_HOME/CollectInsertSizeMetrics.jar MINIMUM_PCT=0 HISTOGRAM_FILE=velvet_PE.pdf  INPUT=velvet_PE.sorted.bam  OUTPUT=velvet_PE.sorted.collectInseSize HISTOGRAM_WIDTH=500
```

Have a look to:

- `velvet_PE.sorted.collectInseSize`
- `velvet_PE.pdf`

What kind of information this table and plot give us? How can be used to judge the assembly?
Let us do it again for the MP:

```
$ bwa mem -t 8 -M Staphylococcus_aureus.velvet.scf.fasta Staphylococcus_aureus_MP_rc_1.fastq Staphylococcus_aureus_MP_rc_2.fastq > velvet_MP.sam
$ samtools view -Sb velvet_MP.sam -o velvet_MP.bam 
$ samtools sort velvet_MP.bam velvet_MP.sorted
$ samtools index velvet_MP.sorted.bam
$ java -Xmx16g -XX:PermSize=8g -jar $PICARD_HOME/CollectInsertSizeMetrics.jar MINIMUM_PCT=0 HISTOGRAM_FILE=velvet_MP.pdf  INPUT=velvet_MP.sorted.bam  OUTPUT=velvet_MP.sorted.collectInseSize HISTOGRAM_WIDTH=8000
```

What kind of information this table and plot give us? How can be used to judge the assembly?

### 02 - QA tools

[QA-tools](https://github.com/CosteaPaul/qaTools) is an extremely usefull program to have a first, but important, understanding about the quality of the assembly. It is a tool under development but it is important to use it. If you like it, bookmark the repository, as it's name is so generic it can be very hard to find on google!

Just like before we will focus on velvet assembly. It is up to you to try the others and/or your assemblies.

We will work in this directory:

```
~/AV_Exercise/03_QAtools
```

To automatically generate all the files (with the exception of pdf files that need to be generated manully) run the command: `bash run_QAtools.sh`.

Create directory and collect the data:

```
$ mkdir velvet
$ cd velvet
$ ln -s ~/AV_Exercise/assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta .
$ ln -s ~/AV_Exercise/02_read_mapping/velvet/velvet_PE.sorted.bam .
```

We will work only with PE data as, in general, PE data is the one with high coverage and less affacted by problems (e.g, duplicated reads).

We run qaCompute (-q 0 count aligment with 0 quality, -m compute median coverage):

```
$ qaCompute -m -q 0 -i velvet_PE.sorted.bam  velvet.cov
```

This generates the table `velvet.cov` and some nice information is given as output. How to interpret information printed on the screen? What about the table velvet.cov? 

qaCompute does no report any information about the assembly sequences, let use the following script to add information about GC-content:

```
$ ~/AV_Exercise/tools/addCG.pl  Staphylococcus_aureus.velvet.scf.fasta velvet.cov > velvet.cov.gc
```

For each contig we now have the length, the coverage and the GC content. We can produce some 2D plots to inspect the result. We can use the following script to produce 2D plots:

```
$ Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R ASSEMBLER.cov.gc MAX_CTG_LENGTH
```

you need to play around with MAX_CTG_LENGTH value to have a nice visualisation. If you have experience with R copy the script and play around.

These are the commands used to generated the pdf files saved in your directories:

```
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R abyss.cov.gc 100
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R allpaths.cov.gc 200
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R masurca.cov.gc 170
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R soapdenovo.cov.gc 100
Rscript --vanilla /proj/g2015027/assemblyValidation/tools/scripts/plotCovGC.R velvet.cov.gc 50
```

#### Questions

- What can you say about the contigs with near-zero coverge?
- Is there a correlation between the length of the contig and the coverage? Why/why not?
- Can you start to have a feeling about quality? 

### 03 - KAT

[KAT](https://github.com/TGAC/KAT) is Kmer Analysis Toolkit, which is very useful in evaluating kmer statistics for assemblies and read sets. We will use it to [...]

Start by going into the KAT directory:

```
$ cd ~/glob/AV_Exercise/04_KAT
```

We will create a file of all input data combined, as KAT only takes a single input file.

```
$ ALLREADS="combined_reads.fastq"
$ READS="../data/*/*"
$ cat $READS >$ALLREADS
```

We will then load the required modules, and run KAT like this on velvet:

```
$ module load bioinfo-tools KAT/2.0.6 gnuplot/4.6.5
$ kat comp -t 8 -C -o reads_vs_velvet combined_reads.fastq ../assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta
$ kat plot spectra-cn -o reads_vs_velvet.png reads_vs_velvet-main.mx
$ kat sect -t 8 -o reads_vs_velvet_sect -C ../assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta combined_reads.fastq
```

This creates the files `reads_vs_velvet-main.mx.spectra-cn.png` and `reads_vs_velvet.png`. Take a look at these!

#### Questions

- Is there anything you can tell from these pictures? What does the black and red area represent in these plots?

### FRCbam

[FRCbam](https://github.com/vezzi/FRC_align) takes as input one or two alignments (usually one PE and one MP library) and looks for suspicious regions where the alignments do not agree with the expected behaviour of the library. As an example places in the assembly where many reads align as singletons, or areas characterised by a too high or too low coverage.
FRCbam acknowledges the trade-off between long contigs/scaffolds and assembly errors, and will try to find the "best" assembly, using a penalty for potential errors while still acknowledging the benefit of contiguity.

We will now compute FRCurve for velvet assembly as example and we will plot the FRCurve for all the other assemblies.You need to work in the following directory:

```
~/glob/AV_Exercise/05_FRCbam
```

run the command `run_FRCbam.sh` to generate the data necessary to produce FRCurves

Let us see in details how it works: 

```
$ mkdir velvet
$ cd velvet
$ ln -s ~/AV_Exercise/02_assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta .
$ ln -s ~/AV_Exercise/04_align/velvet/PE_on_velvet_sorted.bam .
$ ln -s ~/AV_Exercise/04_align/velvet/MP_on_velvet_sorted.bam .
```

And now compute the FRCurve:

```
FRC --pe-sam PE_on_velvet_sorted.bam --pe-max-insert 260 --mp-sam MP_on_velvet_sorted.bam --mp-max-insert 5000 --genome-size 2900000 --output velvet
```

- What information can you get from the printed output? I know it is a bit messy but there is a lot of nice information!!!
- How many features (i.e., suspicious positions) are there in the assembly?
- What are the produced files? The most important one, and the only we will use is velvet_FRC.txt

Let us now compare different assemblies usinf FRC, maybe also your. **Note**: to compare different FRCurve you MUST specify the same genome size, otherwise the curves cannot be compared.

Use the following command from folder `~/AV_Exercise/05_FRCbam/` to plot all 5 FRCurves 

```
$ mkdir FRCplot
$ cd FRCplot
$ Rscript ~/glob/AV_Exercise/tools/plotFRCurves.R ../abyss/abyss_FRC.txt ../allpaths/allpaths_FRC.txt ../masurca/masurca_FRC.txt ../soapdenovo/soapdenovo_FRC.txt ../velvet/velvet_FRC.txt
```

First plot is the entire picture, in order to allow a better visualisation the second plot is a zoom.

- What do you think about ABySS? And Velvet?
- What is the best assembler accordingly to FRCbam? (i.e., the assembly that given a certain amount of features reconstructs the largest portion of the genome....)
- Who is the best between MaSuRCA and Allpaths-LG?

Play a bit around:
- plot FRCurve for a single feature (you only need to change a bit the R script.... in case you do not know nothing about R this is a great moment to start!!!!)
- rerun FRCuve using only PE data, does the scenario changes radically? 

###  REAPR

[Reapr](http://www.sanger.ac.uk/resources/software/reapr/) is a tool trying to find explicit errors in the assembly based on incongruently mapped reads. It is heavily based on too low span coverage, or reads mapping too far or too close to each other. The program will also break up contigs/scaffolds at spurious sites to form smaller (but hopefully correct) contigs.

Like always we will see how to run Reapr on velvet assembly. You nee to work in the folllwoing direcotry

```
~/AV_Exercise/06_REAPR/
```

REAPR is slow, and **currently won't run due to an uppmax problem**, so copy the results from the completed exercises. The script `run_REAPR.sh` would normally run all the analyses, but it takes more than 30 minutes.

The details look like this:

```
$ mkdir velvet
$ cd velvet
$ ln -s ~/AV_Exercise/02_assemblies/velvet/Staphylococcus_aureus.velvet.scf.fasta .
$ ln -s ~/AV_Exercise/04_align/velvet/PE_on_velvet_sorted.bam .
$ reapr pipeline Staphylococcus_aureus.velvet.scf.fasta PE_on_velvet_sorted.bam reapr_velvet
```

Now look at this file: `reapr_velvet/05.summary.report.txt`

- How many assembly errors did Reapr find and break? How many warnings?
- What is the N50 before and after Reapr?
- Do the numbers coincide with FRCbam results?
- Look to the other assemblers... what do you think is the best assembler for Reapr? Why?

### CEGMA and BOSCO

[CEGMA](http://korflab.ucdavis.edu/datasets/cegma/) performs a HMM alignment of 248 Eukaryotic core genes to the assembly. CEGMA reports the completeness. I've allready run this analysis on the assembly to save time.

The software is pretty unstable and is more or less deprecated, it has been used extensively until this year though, so it's definitely worth knowing about! It can be used on UPPMAX (it is installed as a module) but it is pretty slow. We will only look at the reusults (anyway you will find the runCEGMA.sh script to see how to tun it, it takes more than one hour running all 5 assemblies in parallel). 

```
~/AV_Exercise/07_CEGMA/ 
```

Look at this file:

```
~/glob/AV_Exercise/07_CEGMA/velvet/Staphylococcus_aureus.velvet.scf.fasta.cegma.completeness_report
```

- How many complete (>70% aligned) core genes are there in the assembly, according to the CEGMA output? How many partial (>30% aligned)? 

CEGMA was more or less succeeded by a new program called [BOSCO](http://busco.ezlab.org/), which is currently not available on UPPMAX, but as we're test-running it at the moment it should be made available within too long.
